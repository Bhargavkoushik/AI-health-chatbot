from typing import Optional
from functools import lru_cache

try:
    from transformers import pipeline
except Exception:
    pipeline = None

# map short language codes to HF models. Add more as needed.
_MODEL_MAP = {
    ("hi", "en"): "Helsinki-NLP/opus-mt-hi-en",
    ("mr", "en"): "Helsinki-NLP/opus-mt-mr-en",
    ("te", "en"): "Helsinki-NLP/opus-mt-te-en",
    ("en", "hi"): "Helsinki-NLP/opus-mt-en-hi",
    ("en", "mr"): "Helsinki-NLP/opus-mt-en-mr",
    ("en", "te"): "Helsinki-NLP/opus-mt-en-te",
}

@lru_cache(maxsize=8)
def _get_pipeline(src: str, tgt: str):
    if pipeline is None:
        raise RuntimeError("transformers not installed")
    model = _MODEL_MAP.get((src, tgt))
    if model is None:
        raise ValueError(f"No direct HF model for {src}->{tgt}")
    return pipeline("translation", model=model)

def translate(text: str, src_lang: str, tgt_lang: str) -> str:
    """
    Translate text from src_lang to tgt_lang.
    Falls back to returning original if pipeline fails.
    """
    if src_lang == tgt_lang:
        return text
    try:
        p = _get_pipeline(src_lang, tgt_lang)
        out = p(text, max_length=1024)
        return out[0]["translation_text"]
    except Exception:
        # Fallback: return text unchanged (or integrate Google Translate API here)
        return text